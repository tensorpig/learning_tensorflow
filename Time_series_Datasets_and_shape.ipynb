{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time series Datasets and shape",
      "provenance": [],
      "authorship_tag": "ABX9TyORoOKYwCQP9goxu79MdgnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tensorpig/learning_tensorflow/blob/master/Time_series_Datasets_and_shape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bIIxuTqyHpn",
        "colab_type": "text"
      },
      "source": [
        "Let's assume we have a time series data, like share prices or sunspot activity. Then you'll find plenty of sample code on how to use a Tensorflow Dataset to window/batch the time series into consecutive x,y values to use to train your neural network. I'll go through those steps here, looking at the resulting elements of the dataset, and the shape of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brsMuI_-0PHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "series = [1,2,3,4,5] # very simple, dummy time series\n",
        "WINDOW_SIZE=3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAQss_d0zWh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f787fc9a-ad6f-4399-e3c2-3f207f66bc78"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=False)\n",
        "for win in ds:\n",
        "  print(list(win.as_numpy_iterator()))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n",
            "[2, 3, 4]\n",
            "[3, 4, 5]\n",
            "[4, 5]\n",
            "[5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuyjJiixzFro",
        "colab_type": "text"
      },
      "source": [
        "We can use a sliding window on the time series to generate input Xs. If we set drop_remainder to False, the windows taper off when data runs out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyHGrF2qhDMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "906fbee7-266f-45a7-b371-7f54294789ca"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
        "for win in ds:\n",
        "  print(list(win.as_numpy_iterator()))\n",
        "print(ds.element_spec)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n",
            "[2, 3, 4]\n",
            "[3, 4, 5]\n",
            "DatasetSpec(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorShape([]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmPjmO3Vzdzo",
        "colab_type": "text"
      },
      "source": [
        "To keep all Xs uniform we use drop_remainder=True. Note that the resulting WindowDataSet has lost all notion of the shape of the data and each element is a complicated construct of DatasetSpec wrapping around TensorSpec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keUc24F531D0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3b15acea-6317-4522-92df-46ab1b2d9a7b"
      },
      "source": [
        "ds = ds.flat_map(lambda w: w.batch(WINDOW_SIZE))\n",
        "for win in ds:\n",
        "  print(win)\n",
        "print(ds.element_spec)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "tf.Tensor([2 3 4], shape=(3,), dtype=int32)\n",
            "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
            "TensorSpec(shape=(None,), dtype=tf.int32, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798lDYW0z19T",
        "colab_type": "text"
      },
      "source": [
        "Flattening the dataset using a lambda that (again) batches the data in windows results in a somewhat simpler element consistig of a Tensor(Spec). But again, the shape of the data is lost. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3h2lan4CW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f416e6d6-acfd-46a4-ae66-2b735b72b197"
      },
      "source": [
        "ds = ds.batch(2)\n",
        "print(ds.element_spec)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorSpec(shape=(None, None, None), dtype=tf.int32, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU8UYqOJ1Km9",
        "colab_type": "text"
      },
      "source": [
        "A final step is typically batching the data so that the dataset pipeline will feed batches of data into. Note that the shape of the data remains \"lost\" which is consufing when trying to feed this data into a Dense layer, which requires the input_size to be defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gff06sf2dIF",
        "colab_type": "text"
      },
      "source": [
        "I was puzzled why the shape of the data was getting lost, as an iteration through the elements clearly showed that the dataset could know the shape. But then I realised that the Dataset upfront doesn't \"have\" the data and only gets it while iterating. And the 2 batch() calls made on the data as part of the pipeline were made without specifying the drop_remainder=True, meaning the Dataset had no way of knowing upfront if it would end up with partial windows, and hence no way of knowing the data shape. Note: we know of course that there are no partial windows, but the Dataset does not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9MPXvec0qWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "66621ea3-38f5-4114-eb2c-8200f675efe9"
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "ds = ds.window(WINDOW_SIZE + 1, shift=1, drop_remainder=True)\n",
        "ds = ds.flat_map(lambda w: w.batch(WINDOW_SIZE + 1, drop_remainder=True))\n",
        "print(ds)\n",
        "ds = ds.batch(2, drop_remainder=True)\n",
        "print(ds)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FlatMapDataset shapes: (4,), types: tf.int32>\n",
            "<BatchDataset shapes: (2, 4), types: tf.int32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxmBEZD63Dtg",
        "colab_type": "text"
      },
      "source": [
        "Adding the drop_remainder=True to the 2 batch() calls clearly resolves the problem and allows the Dataset to know the shape of the data. I found this much easier to feed into the neural network Dense layer now."
      ]
    }
  ]
}